---
title: "DATA 621 Week 4"
author: "Vinicio Haro"
date: "7/7/2018"
output:
  word_document: default
  html_document: default
---

Our goal is to build a multiple regression model and a logistic regression model that will predict the probability of someone crashing their car and the cost if the car crashes. We are given a dataset with two response variables TARGET_FLAG and TARGET_AMT. TARGET_FLAG is a binary field taking the values 1=crash or 0=no crash while TARGET_AMT is the amount of time spent on repairs given there was a crash. The type of data contained in each response variable is connected to the type of model. TARGET_FLAG is the response variable for logistic regression and TARGET_AMT is the response variable for multiple regression.

The other variables are defined as follows:
AGE Age of Driver Very young people tend to be risky. Maybe very old people also.
BLUEBOOK Value of Vehicle Unknown effect on probability of collision, but probably effect the payout if there is a crash
CAR_AGE Vehicle Age Unknown effect on probability of collision, but probably effect the payout if there is a crash
CAR_TYPE Type of Car Unknown effect on probability of collision, but probably effect the payout if there is a crash
CAR_USE Vehicle Use Commercial vehicles are driven more, so might increase probability of collision
CLM_FREQ # Claims (Past 5 Years) The more claims you filed in the past, the more you are likely to file in the future
EDUCATION Max Education Level Unknown effect, but in theory more educated people tend to drive more safely
HOMEKIDS # Children at Home Unknown effect
HOME_VAL Home Value In theory, home owners tend to drive more responsibly
INCOME Income In theory, rich people tend to get into fewer crashes
JOB Job Category In theory, white collar jobs tend to be safer
KIDSDRIV # Driving Children When teenagers drive your car, you are more likely to get into crashes
MSTATUS Marital Status In theory, married people drive more safely
MVR_PTS Motor Vehicle Record Points If you get lots of traffic tickets, you tend to get into more crashes
OLDCLAIM Total Claims (Past 5 Years) If your total payout over the past five years was high, this suggests future payouts will be high
PARENT1 Single Parent Unknown effect
RED_CAR A Red Car Urban legend says that red cars (especially red sports cars) are more risky. Is that true?
REVOKED License Revoked (Past 7 Years) If your license was revoked in the past 7 years, you probably are a more risky driver.
SEX Gender Urban legend says that women have less crashes then men. Is that true?
TIF Time in Force People who have been customers for a long time are usually more safe.
TRAVTIME Distance to Work Long drives to work usually suggest greater risk
URBANICITY Home/Work Area Unknown
YOJ Years on Job People who stay at a job for a long time are usually more safe

Our plan is the pick the best multiple regression and logistic regression model that best satisfies our goal. This study will be partitioned into four sections starting with EDA, Data preperation, Model Building, and Model Selection.  


I) EDA

We need to partition the data into two subsets. The first subset only takes TARGET_FLAG response and the other variables. The second subset takes TARGET_AMT where TARGET_FLAG=1 in addition to the target variables. 

Read in the data 
```{r, echo=FALSE}
crash_training <- read.csv(url("https://raw.githubusercontent.com/vindication09/DATA-621-Week-4/master/insurance_training_data.csv"), header =TRUE)

crash_evaluation <- read.csv(url("https://raw.githubusercontent.com/vindication09/DATA-621-Week-4/master/insurance-evaluation-data.csv"), header =TRUE)

head(crash_training, 10)
```

How many rows and variables does our data have overall?
```{r, echo=FALSE}
nrow(crash_training);ncol(crash_training);names(crash_training)
```

There are 8,161 rows and 26 variables. The INDEX variable is simply a row number and will not be included in any portion of the analysis from here on out. Each record represents a car owner.

Lets examine our response variables. 
TARGET_FLAG 
```{r, echo=FALSE}
table(crash_training$TARGET_FLAG);

barplot(table(crash_training$TARGET_FLAG), ylim=c(0, 7000), xlab="Result", ylab="N", col="black")
```

The data contains information for roughly 73% non-crashes and 27% crashes. 

TARGET_AMT
```{r, echo=FALSE}
x <- crash_training$TARGET_AMT
h<-hist(x, breaks=10, col="red", xlab="Target AMT", 
    main="Histogram with Normal Curve", ylim=range(0:1200)) 
xfit<-seq(min(x),max(x),length=40) 
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
yfit <- yfit*diff(h$mids[1:2])*length(x) 
lines(xfit, yfit, col="blue", lwd=2);

x <- log(crash_training$TARGET_AMT)
h<-hist(x, breaks=10, col="red", xlab="Target AMT", 
    main="Histogram with Normal Curve After Log Transform", ylim=range(0:1200)) 
#xfit<-seq(min(x),max(x),length=40) 
#yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
#yfit <- yfit*diff(h$mids[1:2])*length(x) 
#lines(x, yfit, col="blue", lwd=2)
```

Almost immediatly, we notice that there is no association with normality however if we add a log transform, the spread of the response variable gets closer to normal. 

General Data Summaries
```{r, echo=FALSE}
summary(crash_training)
```


Lets select the variables that can be reasonably visualized and plot their spreads. We are unable to do a facet plot of all the remaining variables at once since attributes are not identical across measure variables. We produce plots detailing the distribution of the discrete value variables. 
```{r, echo=FALSE}
barplot(table(crash_training$AGE), ylim=c(0, 500), xlab="AGE", ylab="N", col="black");
#barplot(table(crash_training$BLUEBOOK), ylim=c(0, 100), xlab="BLUEBOOK", ylab="N", col="black");
barplot(table(crash_training$CAR_AGE), ylim=c(0, 1000), xlab="CAR_AGE", ylab="N", col="black");
barplot(table(crash_training$CAR_TYPE), ylim=c(0, 5000), xlab="CAR_TYPE", ylab="N", col="black");
barplot(table(crash_training$CAR_USE), ylim=c(0, 5000), xlab="CAR_USE", ylab="N", col="black");
barplot(table(crash_training$CLM_FREQ), ylim=c(0, 5000), xlab="CLM_FREQ", ylab="N", col="black");
barplot(table(crash_training$EDUCATION), ylim=c(0, 5000), xlab="EDUCATION", ylab="N", col="black");
barplot(table(crash_training$HOMEKIDS), ylim=c(0, 7000), xlab="HOMEKIDS", ylab="N", col="black");
#barplot(table(crash_training$HOME_VAL), ylim=c(0, 500), xlab="HOME_VAL", ylab="N", col="black");
#barplot(table(crash_training$INCOME), ylim=c(0, 5000), xlab="INCOME", ylab="N", col="black");
barplot(table(crash_training$JOB), ylim=c(0, 5000), xlab="JOB", ylab="N", col="black");
barplot(table(crash_training$KIDSDRIV), ylim=c(0, 7000), xlab="KIDSDRIV", ylab="N", col="black");
barplot(table(crash_training$MSTATUS), ylim=c(0, 5000), xlab="MSTATUS", ylab="N", col="black");
barplot(table(crash_training$MVR_PTS), ylim=c(0, 4000), xlab="MVR_PTS", ylab="N", col="black");
#barplot(table(crash_training$OLDCLAIM), ylim=c(0, 5), xlab="OLDCLAIM", ylab="N", col="black");
barplot(table(crash_training$PARENT1), ylim=c(0, 5000), xlab="PARENT 1", ylab="N", col="black");
barplot(table(crash_training$RED_CAR), ylim=c(0, 5000), xlab="RED_CAR", ylab="N", col="black");
barplot(table(crash_training$REVOKED), ylim=c(0, 5000), xlab="REVOKED", ylab="N", col="black");
barplot(table(crash_training$SEX), ylim=c(0, 5000), xlab="SEX", ylab="N", col="black");
barplot(table(crash_training$TIF), ylim=c(0, 5000), xlab="TIF", ylab="N", col="black");
barplot(table(crash_training$TRAVTIME), ylim=c(0, 400), xlab="TRAV TIME", ylab="N", col="black");
barplot(table(crash_training$URBANICITY), ylim=c(0, 8000), xlab="URBAN CITY", ylab="N", col="black");
barplot(table(crash_training$YOJ), ylim=c(0, 2000), xlab="YOJ", ylab="N", col="black")


```

From our visuals, car age is the variable that is most alarming to observe. It seems to be that there are negative values but that is not possible for car age. This variable might have outliers caused by errors within the data collection. Travel time also has a very significant outlier but it is related to travele time. It does not seem reasonable that many users would have a travel time of just 5 minutes. YOJ also stands out since this variable is defined as the number of years people stay on the job. There is a visible peak at 0 years but does that group in people who stayed in their jobs for x amunt of months?

Some of these variables might be good candidates for dummy coding especially when doing the linear regression model on them. 
Lets look at the density plot of variables relating to income. We need to convert from factor to numeric in order to visualize.
```{r, echo=FALSE}
library(ggplot2)
library(tidyr)

crash_training_plot_subset <- subset(crash_training, select = c(BLUEBOOK, HOME_VAL,INCOME, OLDCLAIM))
crash_training_plot_subset$BLUEBOOK<-as.numeric(crash_training_plot_subset$BLUEBOOK)
crash_training_plot_subset$HOME_VAL<-as.numeric(crash_training_plot_subset$HOME_VAL)
crash_training_plot_subset$INCOME<-as.numeric(crash_training_plot_subset$INCOME)
crash_training_plot_subset$OLDCLAIM<-as.numeric(crash_training_plot_subset$OLDCLAIM)


ggplot(gather(crash_training_plot_subset), aes(value)) + 
    geom_histogram(bins = 10) + 
    facet_wrap(~key, scales = 'free_x') ;summary(crash_training_plot_subset)
```

Out of these four predictors, home value and income have their largest value of zero. Having zero as income could be due to unemployment and having zero as home value could be due to not being a homeowner. 

In order to perform correlation analysis using standard pearson, we should only consider variables with numeric entries. We will make such a subset and then consider some alternatives to see if we can quantify the correlation of categorical variables. 

How does each variable correlate with TARGET_AMT
```{r, echo=FALSE}
crash_training2<-crash_training
crash_training2<-subset(crash_training, select=-c(INDEX))
crash_evaluation2<-subset(crash_evaluation, select=-c(INDEX))

crash_train_cor<-subset(crash_training2, select=c(TARGET_FLAG, TARGET_AMT, KIDSDRIV, AGE, HOMEKIDS, YOJ, HOME_VAL, TRAVTIME, BLUEBOOK, TIF, OLDCLAIM, CLM_FREQ, MVR_PTS, CAR_AGE))

crash_train_cor$TARGET_AMT<-as.numeric(crash_train_cor$TARGET_AMT)
crash_train_cor$TARGET_FLAG<-as.numeric(crash_train_cor$TARGET_FLAG)
crash_train_cor$KIDSDRIV<-as.numeric(crash_train_cor$KIDSDRIV)
crash_train_cor$AGE<-as.numeric(crash_train_cor$AGE)
crash_train_cor$HOMEKIDS<-as.numeric(crash_train_cor$HOMEKIDS)
crash_train_cor$YOJ<-as.numeric(crash_train_cor$YOJ)
crash_train_cor$HOME_VAL<-as.numeric(crash_train_cor$HOME_VAL)
crash_train_cor$TRAVTIME<-as.numeric(crash_train_cor$TRAVTIME)
crash_train_cor$BLUEBOOK<-as.numeric(crash_train_cor$BLUEBOOK)
crash_train_cor$TIF<-as.numeric(crash_train_cor$TIF)
crash_train_cor$OLDCLAIM<-as.numeric(crash_train_cor$OLDCLAIM)
crash_train_cor$CLM_FREQ<-as.numeric(crash_train_cor$CLM_FREQ)
crash_train_cor$MVR_PTS<-as.numeric(crash_train_cor$MVR_PTS)
crash_train_cor$CAR_AGE<-as.numeric(crash_train_cor$CAR_AGE)


apply(crash_train_cor,2,  function(col)cor(col, crash_training$TARGET_AMT))
```

The two responses have a strong positive correlation. This does not seem out of the ordinary since a car crash needs to occur in order to record the cost of the crash. The response variable has a strong correlation with KIDSDRIV which is quite interesting. Could it be that kid drivers are more accident prone? 

How does each variable correlate with TARGET_FLAG?
```{r, echo=FALSE}
apply(crash_train_cor,2,  function(col)cor(col, crash_training$TARGET_FLAG))
```

The highest correlation with target flag occurs with mvr_pts closely followed by claim_frequency. A value of 1 in target flag is an indicator for a crash. If a car has crashed then that must be put into an insurance claim and contribute to your mvr points. 

How do variables correlate with each other?
```{r, echo=FALSE}
#correlation matrix and visualization 
correlation_matrix <- round(cor(crash_train_cor),2)

# Get lower triangle of the correlation matrix
  get_lower_tri<-function(correlation_matrix){
    correlation_matrix[upper.tri(correlation_matrix)] <- NA
    return(correlation_matrix)
  }
  # Get upper triangle of the correlation matrix
  get_upper_tri <- function(correlation_matrix){
    correlation_matrix[lower.tri(correlation_matrix)]<- NA
    return(correlation_matrix)
  }
  
  upper_tri <- get_upper_tri(correlation_matrix)



library(reshape2)

# Melt the correlation matrix
melted_correlation_matrix <- melt(upper_tri, na.rm = TRUE)

# Heatmap
library(ggplot2)

ggheatmap <- ggplot(data = melted_correlation_matrix, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 15, hjust = 1))+
 coord_fixed()


#add nice labels 
ggheatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 3) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  axis.text.x=element_text(size=rel(0.8), angle=90),
  axis.text.y=element_text(size=rel(0.8)),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwicrash_training2h = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))

```

There are several variables that have minimal correlation with one another. Some variables are not correlated at all such as TIF.

Check missing data
```{r, echo=FALSE}
#missing data 
colSums(is.na(crash_training2)) #;colSums(is.na(crash_evaluation))
```

Age is missing 0.07% data
YOJ is missing 5.56% data
CAR_AGE is missing 6.24% of data 

5% is generally accepted as the threshhold to remove data but I hesitate in removing YOJ due to its theoretical effect on car saftey. I do not believe that YOJ is suitable for imputing methods because it is possible that someone does not have employment history, hence there is nothing to record. How many zero entries appear in the data?

```{r, echo=FALSE}
colSums(crash_training2 == 0)
```

The amount of zero entires in these variables are not suprising given that it is very reasonable for users not have children or any insurance claims. In most of these variables, a zero value implies a safe driver especially considering both the response variables. 

We skip outlier analysis due to several thought points. While we observed some variables with extreme values in the histograms, the extreme values do not seem out of the ordinary. A prime example of an extreme value is travel time being 5 minutes. I believe 5 minutes by car is a reasonable commute time for a good majoirty of people who live close to where they work. Car age is the only variable that does not seem right. Lets run an outlier analysis on car age. 

```{r, echo=FALSE}
outlierKD<-function(crash_training2, var) {
     var_name <- eval(substitute(var),eval(crash_training2))
     na1 <- sum(is.na(var_name))
     m1 <- mean(var_name, na.rm = T)
     par(mfrow=c(2, 2), oma=c(0,0,3,0))
     boxplot(var_name, main="With outliers")
     hist(var_name, main="With outliers", xlab=NA, ylab=NA)
     outlier <- boxplot.stats(var_name)$out
     mo <- mean(outlier)
     var_name <- ifelse(var_name %in% outlier, NA, var_name)
     boxplot(var_name, main="Without outliers")
     hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
     title("Outlier Check", outer=TRUE)
     na2 <- sum(is.na(var_name))
     cat("Outliers identified:", na2 - na1, "n")
     cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "n")
     cat("Mean of the outliers:", round(mo, 2), "n")
     m2 <- mean(var_name, na.rm = T)
     cat("Mean without removing outliers:", round(m1, 2), "n")
     cat("Mean if we remove outliers:", round(m2, 2), "n")
     response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
     if(response == "y" | response == "yes"){
          crash_training2[as.character(substitute(var))] <- invisible(var_name)
          assign(as.character(as.list(match.call())$crash_training2), crash_training2, envir = .GlobalEnv)
          cat("Outliers successfully removed", "n")
          return(invisible(crash_training2))
     } else{
          cat("Nothing changed", "n")
          return(invisible(var_name))
     }
}


outlierKD(crash_training2, CAR_AGE)
```

There does not seem to be a major change in car age with or without outliers. 

II Data Preperation)

There are several variables that are prime candidates for dummy coding such as sex or m status however this may prove to be a long process. We can convert the variables to factors and the lm function will dummy code these categorical variables automatically. 

Recall the missing values 
```{r, echo=FALSE}
colSums(is.na(crash_training2)) #;colSums(is.na(crash_evaluation))
```

After giving it more thought, we need to take into consideration that the data represents policy holders. Age of the policy holder should be imputed using the median age of a policy holder. YOJ is the number of years on a job. It is possible that a policy holder just started a job, or they are not employed. If they are not employed, then there are certain insurance companies that still provide low to no cost healthcare such as Fidelis. The blank entries could be replaced with zero. Car age had the most number of missing values and a strange negative number. The negative number could be due to an erorr with data collection. As for the missing values, we can impute with the median since a car age should not be blank for a policy holder's record. We apply the same transformations to the evaluation data set. 

Impute missing values 
```{r, include=FALSE}
library(Hmisc)

crash_training3<-crash_training2

impute(crash_training3$AGE, median)
impute(crash_training3$CAR_AGE, median)

crash_training3[is.na(crash_training3)] <- 0

colSums(is.na(crash_training3)) 

crash_evaluation3<-crash_evaluation2
impute(crash_evaluation3$AGE, median)
impute(crash_evaluation3$CAR_AGE, median)

crash_evaluation3[is.na(crash_evaluation3)] <- 0

crash_training3$CAR_AGE<-abs(crash_training3$CAR_AGE)


```

Check if there has been any major change in summary after imputing 
```{r, echo=FALSE}
summary(crash_training3);summary(crash_training2)
```

Imputing missing data does not seem to change the stats of the imputed variables. We also took the absolute value of car age to fix the negative entry. 

We need to do some additional modifications to our data otherwise it will complicate modeling. We need to be sure that income and simialr variables are read in as numeric. We also need to recode categorical variables into dummies. Transforming our data is going to play a key role in optimizing our model building.

Count the number of levels per variable after we treat the income realted variables as numeric
```{r, echo=FALSE}
crash_training3$TARGET_AMT<-as.numeric(crash_training3$TARGET_AMT)
crash_training3$TARGET_FLAG<-as.numeric(crash_training3$TARGET_FLAG)
crash_training3$KIDSDRIV<-as.numeric(crash_training3$KIDSDRIV)
crash_training3$AGE<-as.numeric(crash_training3$AGE)
crash_training3$HOMEKIDS<-as.numeric(crash_training3$HOMEKIDS)
crash_training3$YOJ<-as.numeric(crash_training3$YOJ)
crash_training3$HOME_VAL<-as.numeric(crash_training3$HOME_VAL)
crash_training3$TRAVTIME<-as.numeric(crash_training3$TRAVTIME)
crash_training3$BLUEBOOK<-as.numeric(crash_training3$BLUEBOOK)
crash_training3$TIF<-as.numeric(crash_training3$TIF)
crash_training3$OLDCLAIM<-as.numeric(crash_training3$OLDCLAIM)
crash_training3$CLM_FREQ<-as.numeric(crash_training3$CLM_FREQ)
crash_training3$MVR_PTS<-as.numeric(crash_training3$MVR_PTS)
crash_training3$CAR_AGE<-as.numeric(crash_training3$CAR_AGE)
crash_training3$INCOME<-as.numeric(crash_training3$INCOME)

crash_training3a<-crash_training3

crash_evaluation2$TARGET_AMT<-as.numeric(crash_evaluation2$TARGET_AMT)
crash_evaluation2$TARGET_FLAG<-as.numeric(crash_evaluation2$TARGET_FLAG)
crash_evaluation2$KIDSDRIV<-as.numeric(crash_evaluation2$KIDSDRIV)
crash_evaluation2$AGE<-as.numeric(crash_evaluation2$AGE)
crash_evaluation2$HOMEKIDS<-as.numeric(crash_evaluation2$HOMEKIDS)
crash_evaluation2$YOJ<-as.numeric(crash_evaluation2$YOJ)
crash_evaluation2$HOME_VAL<-as.numeric(crash_evaluation2$HOME_VAL)
crash_evaluation2$TRAVTIME<-as.numeric(crash_evaluation2$TRAVTIME)
crash_evaluation2$BLUEBOOK<-as.numeric(crash_evaluation2$BLUEBOOK)
crash_evaluation2$TIF<-as.numeric(crash_evaluation2$TIF)
crash_evaluation2$OLDCLAIM<-as.numeric(crash_evaluation2$OLDCLAIM)
crash_evaluation2$CLM_FREQ<-as.numeric(crash_evaluation2$CLM_FREQ)
crash_evaluation2$MVR_PTS<-as.numeric(crash_evaluation2$MVR_PTS)
crash_evaluation2$CAR_AGE<-as.numeric(crash_evaluation2$CAR_AGE)
crash_evaluation2$INCOME<-as.numeric(crash_evaluation2$INCOME)

crash_evaluation2a<-crash_evaluation2


library(dplyr)
crash_training3 %>% 
     sapply(levels)
```

The variables JOB and EDUCATION have more than 1 factor. These variables are good candidates for recoding as dummy variables in addition to flattening. This should optimize the computation time of the glm function. We can then remove those variables with many factors.  
```{r, include=FALSE}


crash_training3$CLERICAL <- ifelse(crash_training3$JOB == "Clerical", 1, 0)
crash_training3$DOCTOR <- ifelse(crash_training3$JOB == "Doctor", 1, 0)
crash_training3$HOME_MAKER <- ifelse(crash_training3$JOB == "Home Maker", 1, 0)
crash_training3$LAWYER <- ifelse(crash_training3$JOB == "Lawyer", 1, 0)
crash_training3$MANAGER <- ifelse(crash_training3$JOB == "manager", 1, 0)
crash_training3$PROF <- ifelse(crash_training3$JOB == "Professional", 1, 0)
crash_training3$STUDENT <- ifelse(crash_training3$JOB == "Student", 1, 0)
crash_training3$BLUE_COLLAR <- ifelse(crash_training3$JOB == "z_Blue Collar", 1, 0)

crash_training3$MINIVAN <- ifelse(crash_training3$CAR_TYPE == "minivan", 1, 0)
crash_training3$TRUCK <- ifelse(crash_training3$CAR_TYPE == "Panel Truck", 1, 0)
crash_training3$PICKUP <- ifelse(crash_training3$CAR_TYPE == "Pickup", 1, 0)
crash_training3$SPORTS <- ifelse(crash_training3$CAR_TYPE == "Sports Car", 1, 0)
crash_training3$VAN <- ifelse(crash_training3$CAR_TYPE == "Van", 1, 0)
crash_training3$SUV <- ifelse(crash_training3$CAR_TYPE == "z_SUV", 1, 0)

#do it for evaluation data as well 
crash_evaluation3$CLERICAL <- ifelse(crash_evaluation3$JOB == "Clerical", 1, 0)
crash_evaluation3$DOCTOR <- ifelse(crash_evaluation3$JOB == "Doctor", 1, 0)
crash_evaluation3$HOME_MAKER <- ifelse(crash_evaluation3$JOB == "Home Maker", 1, 0)
crash_evaluation3$LAWYER <- ifelse(crash_evaluation3$JOB == "Lawyer", 1, 0)
crash_evaluation3$MANAGER <- ifelse(crash_evaluation3$JOB == "Manager", 1, 0)
crash_evaluation3$PROF <- ifelse(crash_evaluation3$JOB == "Professional", 1, 0)
crash_evaluation3$STUDENT <- ifelse(crash_evaluation3$JOB == "Student", 1, 0)
crash_evaluation3$BLUE_COLLAR <- ifelse(crash_evaluation3$JOB == "z_Blue Collar", 1, 0)

crash_evaluation3$MINIVAN <- ifelse(crash_evaluation3$CAR_TYPE == "Minivan", 1, 0)
crash_evaluation3$TRUCK <- ifelse(crash_evaluation3$CAR_TYPE == "Panel Truck", 1, 0)
crash_evaluation3$PICKUP <- ifelse(crash_evaluation3$CAR_TYPE == "Pickup", 1, 0)
crash_evaluation3$SPORTS <- ifelse(crash_evaluation3$CAR_TYPE == "Sports Car", 1, 0)
crash_evaluation3$VAN <- ifelse(crash_evaluation3$CAR_TYPE == "Van", 1, 0)
crash_evaluation3$SUV <- ifelse(crash_evaluation3$CAR_TYPE == "z_SUV", 1, 0)

crash_training4<-subset(crash_training3, select=-c(CAR_TYPE, JOB))
crash_evaluation4<-subset(crash_evaluation3, select=-c(CAR_TYPE, JOB))
```


III) Model Building 

We seek to build two different models (Linear regression and binary logisitc regression). In the EDA portion we determined that it is best to apply a log(x+1) transformation on the response variable. The log makes the spread appear closer to normal and the +1 makes sure the values in the field are within the domain of the log function. 

Full Logisitic Regression without target_amt
```{r, echo=FALSE}
library(tidyverse)
library(dplyr)
library(broom)
library(car)


mod2<-glm(TARGET_FLAG~AGE+BLUEBOOK+CAR_AGE+CAR_TYPE+CAR_USE+CLM_FREQ+EDUCATION+HOMEKIDS+HOME_VAL+INCOME+JOB+KIDSDRIV+MSTATUS+MVR_PTS+OLDCLAIM+PARENT1+RED_CAR+REVOKED+SEX+TIF+TRAVTIME+URBANICITY+YOJ, family="binomial",  data=crash_training3a)
summary(mod2);


plot(mod2, which = 4, id.n = 3);

mod2.data <- augment(mod2) %>% 
  mutate(index = 1:n()) ;

car::vif(mod2);

mod2.data %>% top_n(3, .cooksd);

ggplot(mod2.data, aes(index, .std.resid)) + 
  geom_point(aes(color = TARGET_FLAG), alpha = .5) +
  theme_bw();

mod2.data %>% 
  filter(abs(.std.resid) > 3);

library(pscl)
pR2(mod2)
```

There are several variables that can be identified as not being significant. These variables are age, bluebook, car age, homekids, income, red car, and yoj. Our next model will eliminate these predictors but retain the levels. These variables were shown to have minimal correlation in our heatmap. Job can also be eliminated due to the fact that it has a high VIF number. The full model yields a McFadden of 0.2. 

```{r, echo=FALSE}
mod3<-glm(TARGET_FLAG~CAR_TYPE+CAR_USE+CLM_FREQ+EDUCATION+HOMEKIDS+HOME_VAL+KIDSDRIV+MSTATUS+MVR_PTS+OLDCLAIM+PARENT1+REVOKED+SEX+TIF+TRAVTIME+URBANICITY, family="binomial",  data=crash_training3a)
summary(mod3);


plot(mod3, which = 4, id.n = 3);

mod3.data <- augment(mod3) %>% 
  mutate(index = 1:n()) ;

car::vif(mod3);

mod3.data %>% top_n(3, .cooksd);

ggplot(mod3.data, aes(index, .std.resid)) + 
  geom_point(aes(color = TARGET_FLAG), alpha = .5) +
  theme_bw();

mod3.data %>% 
  filter(abs(.std.resid) > 3);

pR2(mod3)
```

The second model has the same McFadden as the first but it is less complicated. The AIC is hovering around the same value. 

Build a model using backstepping 
```{r, echo=FALSE}
library(MASS)
step<-stepAIC(mod3, trace=FALSE)
step$anova
```

Formulate the model built by backstepping 
```{r, echo=FALSE}
mod4<-glm(TARGET_FLAG ~ CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + HOMEKIDS + 
    HOME_VAL + KIDSDRIV + MSTATUS + MVR_PTS + OLDCLAIM + PARENT1 + 
    REVOKED + SEX + TIF + TRAVTIME + URBANICITY, family="binomial",  data=crash_training3a)
summary(mod4);


plot(mod4, which = 4, id.n = 3);

mod4.data <- augment(mod4) %>% 
  mutate(index = 1:n()) ;

car::vif(mod4);

mod4.data %>% top_n(3, .cooksd);

ggplot(mod4.data, aes(index, .std.resid)) + 
  geom_point(aes(color = TARGET_FLAG), alpha = .5) +
  theme_bw();

mod4.data %>% 
  filter(abs(.std.resid) > 3);

pR2(mod4)
```

This model allows us to dive deeper into the levels. Having a car type of a truck or having a high school education does not seem to be significant due to high p values. It is commanly known to be a bad statistical practice to remove levels that are not significant because it will dichotomize the predictor. 

We need to find a linear regression model to predict the amount of money a car crash costs.
Full Model with log transform response variable 
```{r, echo=FALSE}
library(olsrr)
lmod2<-lm(log(TARGET_AMT+1)~AGE+BLUEBOOK+CAR_AGE+CAR_TYPE+CAR_USE+CLM_FREQ+EDUCATION+HOMEKIDS+HOME_VAL+INCOME+JOB+KIDSDRIV+MSTATUS+MVR_PTS+OLDCLAIM+PARENT1+RED_CAR+REVOKED+SEX+TIF+TRAVTIME+URBANICITY,data=crash_training3a)
#summary(lmod2);

summary(lmod2);
par(mfrow=c(2,2))
plot(lmod2)
hist(resid(lmod2), main="Histogram of Residuals");
ols_test_breusch_pagan(lmod2);
vif(lmod2)
```

The full linear model suggests that we eliminate job due to its high VIF number. We can aslo remove age, bluebook, car age, education, homekids, and income due to high p values. Education has multiple levels, most with high p values. 

```{r, echo=FALSE}
lmod3<-lm(log(TARGET_AMT+1)~CAR_TYPE+CAR_USE+CLM_FREQ+HOME_VAL+KIDSDRIV+MSTATUS+MVR_PTS+OLDCLAIM+PARENT1+REVOKED+SEX+TIF+TRAVTIME+URBANICITY+YOJ,data=crash_training3a)
#summary(lmod2);

summary(lmod3);
par(mfrow=c(2,2))
plot(lmod3)
hist(resid(lmod3), main="Histogram of Residuals");
ols_test_breusch_pagan(lmod3);
vif(lmod3)
```

The reduced model has a much lower R squared value, however according to the Breusch Pagan test, the low p-value means there is non-constant variance present. 

How about we build a model with backstepping? 
```{r, echo=FALSE}
#library(MASS)
lstep<-stepAIC(lmod2, trace=FALSE)
lstep$anova
```

Formulate the model obtained through backstepping 
```{r, echo=FALSE}
lmod4<-lm(log(TARGET_AMT + 1) ~ CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + 
    HOMEKIDS + HOME_VAL + JOB + KIDSDRIV + MSTATUS + MVR_PTS + 
    OLDCLAIM + PARENT1 + REVOKED + SEX + TIF + TRAVTIME + URBANICITY + 
    YOJ,data=crash_training3a)
#summary(lmod2);

summary(lmod4);
par(mfrow=c(2,2))
plot(lmod4)
hist(resid(lmod4), main="Histogram of Residuals");
ols_test_breusch_pagan(lmod4);
vif(lmod4)
```

Based on the VIF numbers, we can remove job. 
```{r, echo=FALSE}
lmod5<-lm(log(TARGET_AMT+1) ~ CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + 
    HOMEKIDS + HOME_VAL  + KIDSDRIV + MSTATUS + MVR_PTS + 
    OLDCLAIM + PARENT1 + REVOKED + SEX + TIF + TRAVTIME + URBANICITY + 
    YOJ,data=crash_training3a)
#summary(lmod2);

summary(lmod5);
par(mfrow=c(2,2))
plot(lmod5)
hist(resid(lmod5), main="Histogram of Residuals");
ols_test_breusch_pagan(lmod5);
vif(lmod5)
```

This model has VIF numbers less than 4 for all predictors. The adjusted r square is still hovering 2. As for the coefficients, it appears that having a truck has a negative effect on getting into a crash where as sports cars, suvs, and vans have a positive effect. Education also has a negative effect on crashing. Many of these coefficients seem to make sense with our pre-existing knowledge on what elements might contribute to a car crash. It should also be noted that not having a log transform greatly reduces the adjusted r square. 

IV) Model Selection 

We are going to validate mod4 and lmod5. These two models seemed to be the best performing out of all the linear and logisitc regression models built in this study. 

In order to test the performance of our logitic model, we will partition the training data into a test and validation subset before deploying the model on the evaluation data. 
```{r, echo=FALSE}
#The evaluation data does not contain the target response variable. To test predictive power, we need to partition our training data into a test and evaluation 
library(caret)
Train <- createDataPartition(crash_training3a$TARGET_FLAG, p=0.7, list=FALSE)
train <- crash_training3a[Train, ]
test <- crash_training3a[-Train, ]


#probabilities of prediction 
pred <- predict(mod4, newdata=test, type="response", na.action = na.pass)
scored.probability<-data.frame(as.double(pred))


#probability threshold 
pred_filter <- ifelse(pred > 0.5, 1, 0)
scored.class<-data.frame(as.integer(pred_filter))

#subset actuals
class<-data.frame(subset(test, select=c(TARGET_FLAG)))
class<-as.integer(class$TARGET_FLAG)

#library(plyr)
classes.df<-data.frame(class, scored.class, scored.probability)
str(classes.df)
names(classes.df)
```

Get a confusion matrix
```{r, echo=FALSE}
library(pROC)
library(caret)

names(classes.df)<-c("class", "scored.class", "scored.probability")

matrix.df2<-with(classes.df, table(scored.class, class)[2:1, 2:1])
matrix.df2
```

```{r, echo=FALSE}
#confusion matrix
caret_matrix <- confusionMatrix(matrix.df2)
#Information from Confusion matrix
caret_matrix;

f1_reduced<- function(df)
  {
  TP <- sum(classes.df$class == 1 & classes.df$scored.class == 1) 
  Fn <- sum(classes.df$class == 1 & classes.df$scored.class == 0)
  FP <- sum(classes.df$class == 0 & classes.df$scored.class == 1)
  (2*TP)/(2*TP + Fn + FP)
}
f1_reduced(classes.df);

plot(roc(classes.df$class, classes.df$scored.probability), main="ROC Curve");

auc(roc(classes.df$class, classes.df$scored.probability))
```

```{r, echo=FALSE}
summary(mod4)
```

We can convert the coefficients of our model into odds ratios and generate a confidence interval at the 95% confidence level. 
```{r}
exp(cbind("Odds ratio" = coef(mod4), confint.default(mod4, level = 0.95)))
```

From our odds ratios, we can conclude that given a unit increase in a car being a sports car, a user is 3 times more likely to be involved in a crash. Having an education makes someone less likely to be involved in a crash. Not being married makes someone 1.6 times more likely to be in a crash. 

Our logistic regression model yields an accuracy of 0.7872. That is the best performing model we can pick. We need to validate our linear regression model before we deploy on the evaluation data. 

Get predicted values vs actual values and compute root mean square error. 
```{r, echo=FALSE}
lpred <- data.frame(predict(lmod5, newdata=test, type = "response"))
actual <- subset(test, select=c(TARGET_AMT))
l_testing<-data.frame(lpred, actual)
l_testing$rmse <- (mean((l_testing$lpred - l_testing$actual)^2))^0.5
rmse<-(mean((lpred - actual)^2))^0.5
rmse
```

Find relative square error. 
```{r, echo=FALSE}
mu <- mean(actual$TARGET_AMT)
rse <- mean((lpred - actual)^2) / mean((mu - actual)^2)
rse
```

The chosen linear model best minimizes RSME.

We are now in a position to deploy our models on the evaluation data. 
Deploy our linear model 
```{r, echo=FALSE}
test_results <- predict(lmod5, newdata = crash_evaluation2)
head(test_results, 100);
test_results2<-predict(lmod5, newdata=crash_evaluation2, type = "response")
#table(test_results)
#predict(mod5, newdata=moneyball_evaluation3)
#plot(test_results);
#predict(lmod5,newdata=crash_evaluation2, interval='prediction')
```

Deploy the logistic regression model 
```{r, echo=FALSE}
test_resultsb <- predict(mod4, newdata = crash_evaluation2, type="response")
head(test_resultsb, 20)
```

Consolidate and visualize predicted probabilities and predicted amount 
```{r, echo=FALSE, error=TRUE}
predicted_amt<-data.frame(test_results2)
predicted_prb<-data.frame(test_resultsb)
production<-data.frame(predicted_amt,predicted_prb )


xyplot(test_results2 ~ test_results2b, data = production,
  xlab = "Predicted Probabilities",
  ylab = "Predicted Amount",
  main = "Predicted Cost vs. Predicted Probability of Getting Into a Car Crash")
```

Our logisitc regression model had reasonable performance but our linear model had poor predictive power. Rather than using a linear model, I would consider using a different algorithm such as robust regression or smoothing splines. Addative regression might perform better in this scenerio. 

Appendix) 
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

crash_training <- read.csv(url("https://raw.githubusercontent.com/vindication09/DATA-621-Week-4/master/insurance_training_data.csv"), header =TRUE)

crash_evaluation <- read.csv(url("https://raw.githubusercontent.com/vindication09/DATA-621-Week-4/master/insurance-evaluation-data.csv"), header =TRUE)

head(crash_training, 10)



nrow(crash_training);ncol(crash_training);names(crash_training)


table(crash_training$TARGET_FLAG);

barplot(table(crash_training$TARGET_FLAG), ylim=c(0, 7000), xlab="Result", ylab="N", col="black")




x <- crash_training$TARGET_AMT
h<-hist(x, breaks=10, col="red", xlab="Target AMT", 
    main="Histogram with Normal Curve", ylim=range(0:1200)) 
xfit<-seq(min(x),max(x),length=40) 
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
yfit <- yfit*diff(h$mids[1:2])*length(x) 
lines(xfit, yfit, col="blue", lwd=2);

x <- log(crash_training$TARGET_AMT)
h<-hist(x, breaks=10, col="red", xlab="Target AMT", 
    main="Histogram with Normal Curve After Log Transform", ylim=range(0:1200)) 
#xfit<-seq(min(x),max(x),length=40) 
#yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
#yfit <- yfit*diff(h$mids[1:2])*length(x) 
#lines(x, yfit, col="blue", lwd=2)


summary(crash_training)

barplot(table(crash_training$AGE), ylim=c(0, 500), xlab="AGE", ylab="N", col="black");
#barplot(table(crash_training$BLUEBOOK), ylim=c(0, 100), xlab="BLUEBOOK", ylab="N", col="black");
barplot(table(crash_training$CAR_AGE), ylim=c(0, 1000), xlab="CAR_AGE", ylab="N", col="black");
barplot(table(crash_training$CAR_TYPE), ylim=c(0, 5000), xlab="CAR_TYPE", ylab="N", col="black");
barplot(table(crash_training$CAR_USE), ylim=c(0, 5000), xlab="CAR_USE", ylab="N", col="black");
barplot(table(crash_training$CLM_FREQ), ylim=c(0, 5000), xlab="CLM_FREQ", ylab="N", col="black");
barplot(table(crash_training$EDUCATION), ylim=c(0, 5000), xlab="EDUCATION", ylab="N", col="black");
barplot(table(crash_training$HOMEKIDS), ylim=c(0, 7000), xlab="HOMEKIDS", ylab="N", col="black");
#barplot(table(crash_training$HOME_VAL), ylim=c(0, 500), xlab="HOME_VAL", ylab="N", col="black");
#barplot(table(crash_training$INCOME), ylim=c(0, 5000), xlab="INCOME", ylab="N", col="black");
barplot(table(crash_training$JOB), ylim=c(0, 5000), xlab="JOB", ylab="N", col="black");
barplot(table(crash_training$KIDSDRIV), ylim=c(0, 7000), xlab="KIDSDRIV", ylab="N", col="black");
barplot(table(crash_training$MSTATUS), ylim=c(0, 5000), xlab="MSTATUS", ylab="N", col="black");
barplot(table(crash_training$MVR_PTS), ylim=c(0, 4000), xlab="MVR_PTS", ylab="N", col="black");
#barplot(table(crash_training$OLDCLAIM), ylim=c(0, 5), xlab="OLDCLAIM", ylab="N", col="black");
barplot(table(crash_training$PARENT1), ylim=c(0, 5000), xlab="PARENT 1", ylab="N", col="black");
barplot(table(crash_training$RED_CAR), ylim=c(0, 5000), xlab="RED_CAR", ylab="N", col="black");
barplot(table(crash_training$REVOKED), ylim=c(0, 5000), xlab="REVOKED", ylab="N", col="black");
barplot(table(crash_training$SEX), ylim=c(0, 5000), xlab="SEX", ylab="N", col="black");
barplot(table(crash_training$TIF), ylim=c(0, 5000), xlab="TIF", ylab="N", col="black");
barplot(table(crash_training$TRAVTIME), ylim=c(0, 400), xlab="TRAV TIME", ylab="N", col="black");
barplot(table(crash_training$URBANICITY), ylim=c(0, 8000), xlab="URBAN CITY", ylab="N", col="black");
barplot(table(crash_training$YOJ), ylim=c(0, 2000), xlab="YOJ", ylab="N", col="black")



library(ggplot2)
library(tidyr)

crash_training_plot_subset <- subset(crash_training, select = c(BLUEBOOK, HOME_VAL,INCOME, OLDCLAIM))
crash_training_plot_subset$BLUEBOOK<-as.numeric(crash_training_plot_subset$BLUEBOOK)
crash_training_plot_subset$HOME_VAL<-as.numeric(crash_training_plot_subset$HOME_VAL)
crash_training_plot_subset$INCOME<-as.numeric(crash_training_plot_subset$INCOME)
crash_training_plot_subset$OLDCLAIM<-as.numeric(crash_training_plot_subset$OLDCLAIM)


ggplot(gather(crash_training_plot_subset), aes(value)) + 
    geom_histogram(bins = 10) + 
    facet_wrap(~key, scales = 'free_x') ;summary(crash_training_plot_subset)



    crash_training2<-crash_training
crash_training2<-subset(crash_training, select=-c(INDEX))
crash_evaluation2<-subset(crash_evaluation, select=-c(INDEX))

crash_train_cor<-subset(crash_training2, select=c(TARGET_FLAG, TARGET_AMT, KIDSDRIV, AGE, HOMEKIDS, YOJ, HOME_VAL, TRAVTIME, BLUEBOOK, TIF, OLDCLAIM, CLM_FREQ, MVR_PTS, CAR_AGE))

crash_train_cor$TARGET_AMT<-as.numeric(crash_train_cor$TARGET_AMT)
crash_train_cor$TARGET_FLAG<-as.numeric(crash_train_cor$TARGET_FLAG)
crash_train_cor$KIDSDRIV<-as.numeric(crash_train_cor$KIDSDRIV)
crash_train_cor$AGE<-as.numeric(crash_train_cor$AGE)
crash_train_cor$HOMEKIDS<-as.numeric(crash_train_cor$HOMEKIDS)
crash_train_cor$YOJ<-as.numeric(crash_train_cor$YOJ)
crash_train_cor$HOME_VAL<-as.numeric(crash_train_cor$HOME_VAL)
crash_train_cor$TRAVTIME<-as.numeric(crash_train_cor$TRAVTIME)
crash_train_cor$BLUEBOOK<-as.numeric(crash_train_cor$BLUEBOOK)
crash_train_cor$TIF<-as.numeric(crash_train_cor$TIF)
crash_train_cor$OLDCLAIM<-as.numeric(crash_train_cor$OLDCLAIM)
crash_train_cor$CLM_FREQ<-as.numeric(crash_train_cor$CLM_FREQ)
crash_train_cor$MVR_PTS<-as.numeric(crash_train_cor$MVR_PTS)
crash_train_cor$CAR_AGE<-as.numeric(crash_train_cor$CAR_AGE)


apply(crash_train_cor,2,  function(col)cor(col, crash_training$TARGET_AMT))



apply(crash_train_cor,2,  function(col)cor(col, crash_training$TARGET_FLAG))



#correlation matrix and visualization 
correlation_matrix <- round(cor(crash_train_cor),2)

# Get lower triangle of the correlation matrix
  get_lower_tri<-function(correlation_matrix){
    correlation_matrix[upper.tri(correlation_matrix)] <- NA
    return(correlation_matrix)
  }
  # Get upper triangle of the correlation matrix
  get_upper_tri <- function(correlation_matrix){
    correlation_matrix[lower.tri(correlation_matrix)]<- NA
    return(correlation_matrix)
  }
  
  upper_tri <- get_upper_tri(correlation_matrix)



library(reshape2)

# Melt the correlation matrix
melted_correlation_matrix <- melt(upper_tri, na.rm = TRUE)

# Heatmap
library(ggplot2)

ggheatmap <- ggplot(data = melted_correlation_matrix, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 15, hjust = 1))+
 coord_fixed()


#add nice labels 
ggheatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 3) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  axis.text.x=element_text(size=rel(0.8), angle=90),
  axis.text.y=element_text(size=rel(0.8)),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwicrash_training2h = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))



  #missing data 
colSums(is.na(crash_training2)) #;colSums(is.na(crash_evaluation))
colSums(crash_training2 == 0)


outlierKD<-function(crash_training2, var) {
     var_name <- eval(substitute(var),eval(crash_training2))
     na1 <- sum(is.na(var_name))
     m1 <- mean(var_name, na.rm = T)
     par(mfrow=c(2, 2), oma=c(0,0,3,0))
     boxplot(var_name, main="With outliers")
     hist(var_name, main="With outliers", xlab=NA, ylab=NA)
     outlier <- boxplot.stats(var_name)$out
     mo <- mean(outlier)
     var_name <- ifelse(var_name %in% outlier, NA, var_name)
     boxplot(var_name, main="Without outliers")
     hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
     title("Outlier Check", outer=TRUE)
     na2 <- sum(is.na(var_name))
     cat("Outliers identified:", na2 - na1, "n")
     cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "n")
     cat("Mean of the outliers:", round(mo, 2), "n")
     m2 <- mean(var_name, na.rm = T)
     cat("Mean without removing outliers:", round(m1, 2), "n")
     cat("Mean if we remove outliers:", round(m2, 2), "n")
     response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
     if(response == "y" | response == "yes"){
          crash_training2[as.character(substitute(var))] <- invisible(var_name)
          assign(as.character(as.list(match.call())$crash_training2), crash_training2, envir = .GlobalEnv)
          cat("Outliers successfully removed", "n")
          return(invisible(crash_training2))
     } else{
          cat("Nothing changed", "n")
          return(invisible(var_name))
     }
}



outlierKD(crash_training2, CAR_AGE)


colSums(is.na(crash_training2)) #;colSums(is.na(crash_evaluation))



library(Hmisc)

crash_training3<-crash_training2

impute(crash_training3$AGE, median)
impute(crash_training3$CAR_AGE, median)

crash_training3[is.na(crash_training3)] <- 0

colSums(is.na(crash_training3)) 

crash_evaluation3<-crash_evaluation2
impute(crash_evaluation3$AGE, median)
impute(crash_evaluation3$CAR_AGE, median)

crash_evaluation3[is.na(crash_evaluation3)] <- 0

crash_training3$CAR_AGE<-abs(crash_training3$CAR_AGE)



summary(crash_training3);summary(crash_training2)



crash_training3$TARGET_AMT<-as.numeric(crash_training3$TARGET_AMT)
crash_training3$TARGET_FLAG<-as.numeric(crash_training3$TARGET_FLAG)
crash_training3$KIDSDRIV<-as.numeric(crash_training3$KIDSDRIV)
crash_training3$AGE<-as.numeric(crash_training3$AGE)
crash_training3$HOMEKIDS<-as.numeric(crash_training3$HOMEKIDS)
crash_training3$YOJ<-as.numeric(crash_training3$YOJ)
crash_training3$HOME_VAL<-as.numeric(crash_training3$HOME_VAL)
crash_training3$TRAVTIME<-as.numeric(crash_training3$TRAVTIME)
crash_training3$BLUEBOOK<-as.numeric(crash_training3$BLUEBOOK)
crash_training3$TIF<-as.numeric(crash_training3$TIF)
crash_training3$OLDCLAIM<-as.numeric(crash_training3$OLDCLAIM)
crash_training3$CLM_FREQ<-as.numeric(crash_training3$CLM_FREQ)
crash_training3$MVR_PTS<-as.numeric(crash_training3$MVR_PTS)
crash_training3$CAR_AGE<-as.numeric(crash_training3$CAR_AGE)
crash_training3$INCOME<-as.numeric(crash_training3$INCOME)

crash_training3a<-crash_training3

crash_evaluation2$TARGET_AMT<-as.numeric(crash_evaluation2$TARGET_AMT)
crash_evaluation2$TARGET_FLAG<-as.numeric(crash_evaluation2$TARGET_FLAG)
crash_evaluation2$KIDSDRIV<-as.numeric(crash_evaluation2$KIDSDRIV)
crash_evaluation2$AGE<-as.numeric(crash_evaluation2$AGE)
crash_evaluation2$HOMEKIDS<-as.numeric(crash_evaluation2$HOMEKIDS)
crash_evaluation2$YOJ<-as.numeric(crash_evaluation2$YOJ)
crash_evaluation2$HOME_VAL<-as.numeric(crash_evaluation2$HOME_VAL)
crash_evaluation2$TRAVTIME<-as.numeric(crash_evaluation2$TRAVTIME)
crash_evaluation2$BLUEBOOK<-as.numeric(crash_evaluation2$BLUEBOOK)
crash_evaluation2$TIF<-as.numeric(crash_evaluation2$TIF)
crash_evaluation2$OLDCLAIM<-as.numeric(crash_evaluation2$OLDCLAIM)
crash_evaluation2$CLM_FREQ<-as.numeric(crash_evaluation2$CLM_FREQ)
crash_evaluation2$MVR_PTS<-as.numeric(crash_evaluation2$MVR_PTS)
crash_evaluation2$CAR_AGE<-as.numeric(crash_evaluation2$CAR_AGE)
crash_evaluation2$INCOME<-as.numeric(crash_evaluation2$INCOME)

crash_evaluation2a<-crash_evaluation2


library(dplyr)
crash_training3 %>% 
     sapply(levels)



  #modeling
  mod2<-glm(TARGET_FLAG~AGE+BLUEBOOK+CAR_AGE+CAR_TYPE+CAR_USE+CLM_FREQ+EDUCATION+HOMEKIDS+HOME_VAL+INCOME+JOB+KIDSDRIV+MSTATUS+MVR_PTS+OLDCLAIM+PARENT1+RED_CAR+REVOKED+SEX+TIF+TRAVTIME+URBANICITY+YOJ, family="binomial",  data=crash_training3a)
summary(mod2);


plot(mod2, which = 4, id.n = 3);

mod2.data <- augment(mod2) %>% 
  mutate(index = 1:n()) ;

car::vif(mod2);

mod2.data %>% top_n(3, .cooksd);

ggplot(mod2.data, aes(index, .std.resid)) + 
  geom_point(aes(color = TARGET_FLAG), alpha = .5) +
  theme_bw();

mod2.data %>% 
  filter(abs(.std.resid) > 3);

library(pscl)
pR2(mod2)



mod3<-glm(TARGET_FLAG~CAR_TYPE+CAR_USE+CLM_FREQ+EDUCATION+HOMEKIDS+HOME_VAL+KIDSDRIV+MSTATUS+MVR_PTS+OLDCLAIM+PARENT1+REVOKED+SEX+TIF+TRAVTIME+URBANICITY, family="binomial",  data=crash_training3a)
summary(mod3);


plot(mod3, which = 4, id.n = 3);

mod3.data <- augment(mod3) %>% 
  mutate(index = 1:n()) ;

car::vif(mod3);

mod3.data %>% top_n(3, .cooksd);

ggplot(mod3.data, aes(index, .std.resid)) + 
  geom_point(aes(color = TARGET_FLAG), alpha = .5) +
  theme_bw();

mod3.data %>% 
  filter(abs(.std.resid) > 3);

pR2(mod3)


library(MASS)
step<-stepAIC(mod3, trace=FALSE)
step$anova


mod4<-glm(TARGET_FLAG ~ CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + HOMEKIDS + 
    HOME_VAL + KIDSDRIV + MSTATUS + MVR_PTS + OLDCLAIM + PARENT1 + 
    REVOKED + SEX + TIF + TRAVTIME + URBANICITY, family="binomial",  data=crash_training3a)
summary(mod4);


plot(mod4, which = 4, id.n = 3);

mod4.data <- augment(mod4) %>% 
  mutate(index = 1:n()) ;

car::vif(mod4);

mod4.data %>% top_n(3, .cooksd);

ggplot(mod4.data, aes(index, .std.resid)) + 
  geom_point(aes(color = TARGET_FLAG), alpha = .5) +
  theme_bw();

mod4.data %>% 
  filter(abs(.std.resid) > 3);

pR2(mod4)



library(olsrr)
lmod2<-lm(log(TARGET_AMT+1)~AGE+BLUEBOOK+CAR_AGE+CAR_TYPE+CAR_USE+CLM_FREQ+EDUCATION+HOMEKIDS+HOME_VAL+INCOME+JOB+KIDSDRIV+MSTATUS+MVR_PTS+OLDCLAIM+PARENT1+RED_CAR+REVOKED+SEX+TIF+TRAVTIME+URBANICITY,data=crash_training3a)
#summary(lmod2);

summary(lmod2);
par(mfrow=c(2,2))
plot(lmod2)
hist(resid(lmod2), main="Histogram of Residuals");
ols_test_breusch_pagan(lmod2);
vif(lmod2)



lmod3<-lm(log(TARGET_AMT+1)~CAR_TYPE+CAR_USE+CLM_FREQ+HOME_VAL+KIDSDRIV+MSTATUS+MVR_PTS+OLDCLAIM+PARENT1+REVOKED+SEX+TIF+TRAVTIME+URBANICITY+YOJ,data=crash_training3a)
#summary(lmod2);

summary(lmod3);
par(mfrow=c(2,2))
plot(lmod3)
hist(resid(lmod3), main="Histogram of Residuals");
ols_test_breusch_pagan(lmod3);
vif(lmod3)



#library(MASS)
lstep<-stepAIC(lmod2, trace=FALSE)
lstep$anova


lmod4<-lm(log(TARGET_AMT + 1) ~ CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + 
    HOMEKIDS + HOME_VAL + JOB + KIDSDRIV + MSTATUS + MVR_PTS + 
    OLDCLAIM + PARENT1 + REVOKED + SEX + TIF + TRAVTIME + URBANICITY + 
    YOJ,data=crash_training3a)
#summary(lmod2);

summary(lmod4);
par(mfrow=c(2,2))
plot(lmod4)
hist(resid(lmod4), main="Histogram of Residuals");
ols_test_breusch_pagan(lmod4);
vif(lmod4)



lmod5<-lm(log(TARGET_AMT+1) ~ CAR_TYPE + CAR_USE + CLM_FREQ + EDUCATION + 
    HOMEKIDS + HOME_VAL  + KIDSDRIV + MSTATUS + MVR_PTS + 
    OLDCLAIM + PARENT1 + REVOKED + SEX + TIF + TRAVTIME + URBANICITY + 
    YOJ,data=crash_training3a)
#summary(lmod2);

summary(lmod5);
par(mfrow=c(2,2))
plot(lmod5)
hist(resid(lmod5), main="Histogram of Residuals");
ols_test_breusch_pagan(lmod5);
vif(lmod5)


#model selection 
#The evaluation data does not contain the target response variable. To test predictive power, we need to partition our training data into a test and evaluation 
library(caret)
Train <- createDataPartition(crash_training3a$TARGET_FLAG, p=0.7, list=FALSE)
train <- crash_training3a[Train, ]
test <- crash_training3a[-Train, ]


#probabilities of prediction 
pred <- predict(mod4, newdata=test, type="response", na.action = na.pass)
scored.probability<-data.frame(as.double(pred))


#probability threshold 
pred_filter <- ifelse(pred > 0.5, 1, 0)
scored.class<-data.frame(as.integer(pred_filter))

#subset actuals
class<-data.frame(subset(test, select=c(TARGET_FLAG)))
class<-as.integer(class$TARGET_FLAG)

#library(plyr)
classes.df<-data.frame(class, scored.class, scored.probability)
str(classes.df)
names(classes.df)


library(pROC)
library(caret)

names(classes.df)<-c("class", "scored.class", "scored.probability")

matrix.df2<-with(classes.df, table(scored.class, class)[2:1, 2:1])
matrix.df2


#confusion matrix
caret_matrix <- confusionMatrix(matrix.df2)
#Information from Confusion matrix
caret_matrix;

f1_reduced<- function(df)
  {
  TP <- sum(classes.df$class == 1 & classes.df$scored.class == 1) 
  Fn <- sum(classes.df$class == 1 & classes.df$scored.class == 0)
  FP <- sum(classes.df$class == 0 & classes.df$scored.class == 1)
  (2*TP)/(2*TP + Fn + FP)
}
f1_reduced(classes.df);

plot(roc(classes.df$class, classes.df$scored.probability), main="ROC Curve");

auc(roc(classes.df$class, classes.df$scored.probability))


exp(cbind("Odds ratio" = coef(mod4), confint.default(mod4, level = 0.95)))


lpred <- data.frame(predict(lmod5, newdata=test, type = "response"))
actual <- subset(test, select=c(TARGET_AMT))
l_testing<-data.frame(lpred, actual)
l_testing$rmse <- (mean((l_testing$lpred - l_testing$actual)^2))^0.5
rmse


mu <- mean(actual$TARGET_AMT)
rse <- mean((lpred - actual)^2) / mean((mu - actual)^2)
rse


#prediction
test_results <- predict(lmod5, newdata = crash_evaluation2)
head(test_results, 100);
test_results2<-predict(lmod5, newdata=crash_evaluation2, type = "response")
#table(test_results)
#predict(mod5, newdata=moneyball_evaluation3)
#plot(test_results);
predict(lmod5,newdata=crash_evaluation2, interval='prediction')

test_resultsb <- predict(mod4, newdata = crash_evaluation2, type="response")
head(test_resultsb, 20)



predicted_amt<-data.frame(test_results2)
predicted_prb<-data.frame(test_resultsb)
production<-data.frame(predicted_amt,predicted_prb )


xyplot(test_results2 ~ test_results2b, data = production,
  xlab = "Predicted Probabilities",
  ylab = "Predicted Amount",
  main = "Predicted Cost of Car Crash vs. Predicted Probability of Getting Into a Car Crash")






```

